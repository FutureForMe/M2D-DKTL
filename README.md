# M2D-DKTL

This is the official implementation of the paper "**M2D-DKTL: Multi-modal Multi-agent Debate-based Dialectical Knowledge Transfer Learning**".

## Running Experiments

### Step 0: Preparation
<!-- é¦–å…ˆä½ éœ€è¦å®‰è£…ä¸‹é¢çš„ä¾èµ–åº“ã€‚ -->
First, you need to install the following additional packages.

```
torch
openai
transformers
```

<!-- ç„¶åŽä½ éœ€è¦ä¸‹è½½å¯¹åº”çš„æ•°æ®é›†MMMUã€MathVistaå’ŒCMMMUå¯¹åº”çš„å›¾ç‰‡åˆ°å¯¹åº”çš„æ–‡ä»¶å¤¹ã€‚ -->
Next, you need to download the corresponding datasets (MMMU, MathVista, and CMMMU) and the associated images to the appropriate folders.

[[ðŸ¤— MMMU](https://huggingface.co/datasets/MMMU/MMMU)] [[ðŸ¤— MathVista](https://huggingface.co/datasets/AI4Math/MathVista)] [[ðŸ¤— CMMMU](https://huggingface.co/datasets/m-a-p/CMMMU)]

```
|-- data
    |-- images
        |-- MMMU_images
        |-- MathVista_images
        |-- CMMMU_images
```

### Step 1: Multi-modal Multi-agent Debate
<!-- æˆ‘ä»¬ä½¿ç”¨äº†Azure OpenAIæä¾›çš„APIæŽ¥å£ï¼Œä½ éœ€è¦å°†option.pyä¸­çš„RESOURCE_NAMEå’ŒAPI_KEYæ›¿æ¢ä¸ºä½ è‡ªå·±çš„ã€‚æœ€åŽè¿è¡Œä¸‹é¢çš„ä»£ç è¿›è¡Œå¤šæ¨¡æ€å¤šæ™ºèƒ½ä½“è¾©è®ºã€‚ -->
We used the API provided by Azure OpenAI. You need to replace the _RESOURCE_NAME_ and _API_KEY_ in `option.py` with your own. Then, run the following code to conduct a multimodal multi-agent debate.

```
sh scripts/run_multi_agent_debate_mmmu.sh         # MMMU dataset
sh scripts/run_multi_agent_debate_math_vista.sh   # MathVista dataset
```

<!-- ç”±äºŽMMMUå’ŒMathVistaä¸­çš„æ•°æ®æœ‰å¤šé€‰é€‰æ‹©é¢˜å’Œé—®ç­”é¢˜ï¼Œå› æ­¤æˆ‘ä»¬åœ¨å¾—åˆ°è¾©è®ºç»“æžœåŽä½¿ç”¨GPT4å¯¹ç»“æžœè¿›è¡Œè¯„ä¼°ã€‚ä½ å¯ä»¥è¿è¡Œä¸‹é¢çš„ä»£ç å¯¹æ•°æ®è¿›è¡Œè¯„ä¼°ã€‚ -->
Due to the multiple-choice and question-answering tasks in MMMU and MathVista datasets, we use GPT-4 to evaluate the results after obtaining the debate results. You can run the following code to evaluate the data.

```
sh scripts/run_evaluation.sh
```

### Step 2: Construct Chain-of-Debate Data
<!-- åœ¨å¾—åˆ°å¤šæ™ºèƒ½ä½“çš„è¾©è®ºè¿‡ç¨‹ä¹‹åŽï¼Œä½ å¯ä»¥è¿è¡Œä¸‹é¢çš„ä»£ç ä»Žè¾©è®ºç»“æžœä¸­æž„é€ å‡ºé“¾å¼è¾©è®ºæ•°æ®ã€‚ -->
After obtaining the multi-agent debate process, you can run the following code to construct chain-of-debate data from the debate results.

```
python construct_cod_data.py
```

### Step 3: Train Dialectical Knowledge Transfer Learning
<!-- æˆ‘ä»¬å·²ç»å°†æˆ‘ä»¬åŸºäºŽMMMUæ•°æ®é›†å‡ç»ƒå¥½çš„CoDæ•°æ®æ”¾åœ¨äº†æ–‡ä»¶å¤¹data/cod_data/ä¸‹ï¼Œä½ å¯ä»¥ç›´æŽ¥è¿è¡Œä¸‹é¢çš„ä»£ç è®­ç»ƒLLaVAï¼Œè¯·ç¡®ä¿ä½ å·²ç»ä¸‹è½½äº†LLaVAåŽŸå§‹çš„æ¨¡åž‹æƒé‡ã€‚ -->
We have already placed the CoD data distilled from the MMMU dataset in the folder data/cod_data/. You can directly run the following code to train LLaVA. Please ensure that you have already downloaded the original LLaVA-1.5 model weights [[ðŸ¤— LLaVA-1.5](https://huggingface.co/collections/liuhaotian/llava-15-653aac15d994e992e2677a7e)].

```
sh scripts/train_llava_full_7b_13b.sh
```

<!-- åœ¨è®­ç»ƒç»“æŸåŽï¼Œä½ å¯ä»¥è¿è¡Œä¸‹é¢çš„ä»£ç å¯¹æ¨¡åž‹è¿›è¡ŒæŽ¨ç†ï¼ŒæŽ¨ç†ç»“æžœå¯ä»¥ä½¿ç”¨Step 1ä¸­çš„è¯„ä¼°ä»£ç è¿›è¡Œè¯„ä¼°ã€‚ -->
After the training is complete, you can run the following code for inference. The inference results can be evaluated using the evaluation code from Step 1.

```
sh scripts/inference_llava.sh
```

## Acknowledgement
<!-- æˆ‘ä»¬åŸºäºŽLLaVAçš„æºç è¿›è¡Œäº†ä¿®æ”¹ï¼Œæ„Ÿè°¢LLaVAçš„å¼€æºã€‚ -->
[LLaVA](https://github.com/haotian-liu/LLaVA): We made modifications based on the source code of LLaVA, and we would like to thank LLaVA for their open-source contributions.